{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets peft accelerate bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:23:40.042548Z","iopub.execute_input":"2025-06-08T11:23:40.042865Z","iopub.status.idle":"2025-06-08T11:23:44.077240Z","shell.execute_reply.started":"2025-06-08T11:23:40.042814Z","shell.execute_reply":"2025-06-08T11:23:44.076491Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import load_dataset\nfrom huggingface_hub import login\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:23:58.027615Z","iopub.execute_input":"2025-06-08T11:23:58.027885Z","iopub.status.idle":"2025-06-08T11:23:58.031912Z","shell.execute_reply.started":"2025-06-08T11:23:58.027858Z","shell.execute_reply":"2025-06-08T11:23:58.031302Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"login(token='NEED TOKEN')\n\nmodel_name = \"mistralai/Mistral-7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:23:44.084261Z","iopub.execute_input":"2025-06-08T11:23:44.084497Z","iopub.status.idle":"2025-06-08T11:23:44.348399Z","shell.execute_reply.started":"2025-06-08T11:23:44.084478Z","shell.execute_reply":"2025-06-08T11:23:44.347870Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"device = torch.device(\"cuda:0\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_4bit=True\n).to(device)\n\npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    task_type = TaskType.CAUSAL_LM\n)\n\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:24:01.104270Z","iopub.execute_input":"2025-06-08T11:24:01.104854Z","iopub.status.idle":"2025-06-08T11:24:44.546166Z","shell.execute_reply.started":"2025-06-08T11:24:01.104812Z","shell.execute_reply":"2025-06-08T11:24:44.545554Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f95830d5f39041b0907b85aea57a801a"}},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"dataset = load_dataset(\"tatsu-lab/alpaca\")[\"train\"].select(range(1000))\n\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:24:49.355253Z","iopub.execute_input":"2025-06-08T11:24:49.355522Z","iopub.status.idle":"2025-06-08T11:24:50.283541Z","shell.execute_reply.started":"2025-06-08T11:24:49.355504Z","shell.execute_reply":"2025-06-08T11:24:50.282967Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'output', 'text'],\n    num_rows: 1000\n})"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:24:52.099540Z","iopub.execute_input":"2025-06-08T11:24:52.099802Z","iopub.status.idle":"2025-06-08T11:24:52.105448Z","shell.execute_reply.started":"2025-06-08T11:24:52.099783Z","shell.execute_reply":"2025-06-08T11:24:52.104874Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'instruction': 'Give three tips for staying healthy.',\n 'input': '',\n 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.',\n 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"def preprocess(ex):\n    prompt = f\"### Instruction:\\n{ex['instruction']}\\n\\n### Response:\\n{ex['output']}\"\n    tokenized = tokenizer(prompt, truncation=True, padding='max_length', max_length=512)\n    tokenized['labels'] = tokenized['input_ids'].copy()\n    return tokenized\n\ntokenized_dataset = dataset.map(preprocess)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:24:55.165590Z","iopub.execute_input":"2025-06-08T11:24:55.165922Z","iopub.status.idle":"2025-06-08T11:24:55.203867Z","shell.execute_reply.started":"2025-06-08T11:24:55.165899Z","shell.execute_reply":"2025-06-08T11:24:55.203150Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=TrainingArguments(\n        per_device_train_batch_size=1,\n        num_train_epochs=1,\n        learning_rate=2e-4,\n        output_dir=\"./lora-mistral-alpaca\",\n        fp16=True,\n        logging_steps=10,\n        save_steps=100,\n        save_total_limit=2,\n        gradient_accumulation_steps=4,\n        report_to='none'\n    ),\n    train_dataset=tokenized_dataset,\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:26:00.514673Z","iopub.execute_input":"2025-06-08T11:26:00.515391Z","iopub.status.idle":"2025-06-08T12:11:15.606897Z","shell.execute_reply.started":"2025-06-08T11:26:00.515367Z","shell.execute_reply":"2025-06-08T12:11:15.606287Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 44:53, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.789600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.704500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.607100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.726900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.517500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.702800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.530800</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.561800</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.652500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.608100</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.546900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.525000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=125, training_loss=0.6229800281524658, metrics={'train_runtime': 2714.5036, 'train_samples_per_second': 0.368, 'train_steps_per_second': 0.046, 'total_flos': 2.1854416797696e+16, 'train_loss': 0.6229800281524658, 'epoch': 1.0})"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"model.save_pretrained(\"./lora-mistral-alpaca\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T12:15:16.293906Z","iopub.execute_input":"2025-06-08T12:15:16.294627Z","iopub.status.idle":"2025-06-08T12:15:16.524594Z","shell.execute_reply.started":"2025-06-08T12:15:16.294602Z","shell.execute_reply":"2025-06-08T12:15:16.523817Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\nlogin(token='NEED TOKEN')\n\nbase_model_name = \"mistralai/Mistral-7B-v0.1\"  \nlora_model_path = \"./lora-mistral-alpaca\"  # путь к LoRA\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\", load_in_4bit=True)\nmodel = PeftModel.from_pretrained(base_model, lora_model_path)\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T12:15:36.304065Z","iopub.execute_input":"2025-06-08T12:15:36.304562Z","iopub.status.idle":"2025-06-08T12:15:51.112143Z","shell.execute_reply.started":"2025-06-08T12:15:36.304539Z","shell.execute_reply":"2025-06-08T12:15:51.111481Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d9eaf72cc6f4213bd69267afb949994"}},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            )\n            (mlp): MistralMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): MistralRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): MistralRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"from transformers import pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nprompt = \"### Instruction:\\nExplain the difference between supervised and unsupervised learning.\\n\\n### Response:\\n\"\noutputs = pipe(prompt, max_new_tokens=200, do_sample=True, temperature=0.7)\nprint(outputs[0][\"generated_text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T12:16:21.171123Z","iopub.execute_input":"2025-06-08T12:16:21.171389Z","iopub.status.idle":"2025-06-08T12:16:31.660662Z","shell.execute_reply.started":"2025-06-08T12:16:21.171369Z","shell.execute_reply":"2025-06-08T12:16:31.660066Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nThe model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"### Instruction:\nExplain the difference between supervised and unsupervised learning.\n\n### Response:\nSupervised learning is a type of machine learning that uses labeled datasets to train models. Unsupervised learning, on the other hand, uses unlabeled data to extract patterns and features from data. Supervised learning is typically used to solve specific, known problems, while unsupervised learning is used to discover hidden patterns in data. Unsupervised learning is used when there is not enough information to classify data into predefined categories, and is often used in exploratory analysis of data.\n","output_type":"stream"}],"execution_count":34}]}